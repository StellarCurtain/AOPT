# çº¦æŸConstrained
- any optimization problem can be regarded as an unconstrained one
- ![alt text](image.png)
# Convex Optimization
- æœ¬è¯¾ç¨‹ä¸“æ³¨äºä»¥ä¸‹é—®é¢˜è®¾ç½®
- - Language: minimization problem
- - Objective function: continuous and convex
- - Feasible domain: a convex subset of Euclidean space
# Part 1. Convex Set and Convex Function
- **Def1 : Convex Set**
- - $X$ä¸ºå‡¸é›†: for any $x_1, x_2 \in X$, $x_1$ å’Œ $x_2$ è¿æˆçº¿æ®µä¸Šçš„æ‰€æœ‰ç‚¹å±äº $X$
- - $\forall \alpha \in [0, 1], \alpha x_1 + (1 - \alpha) x_2 \in X$
- **Def2 : Ball**
- - $B(x_c, r) = \{x_c + ru | \|u\|_2 \leq 1\}$
- **Def3 : Ellipsoid**
- - $\epsilon(x_c, A) = \{x_c + Au | \|u\|_2 \leq 1\}$
- **Def4 : Convex Hull**
- - é›†åˆXçš„å‡¸åŒ…$conv X = \{\sum_{i=1}^n \alpha_i x_i | x_i \in X, \alpha_i \geq 0, \sum_{i=1}^n \alpha_i = 1\}$
- **Def5: Projection**
- - yåœ¨é›†åˆXä¸Šçš„æŠ•å½±$x = \Pi _X(y) = argmin_{x\in X} \|x - y\|_2$
- **Theorem 1: (Pythagoras Theorem)**
- - å¯¹å‡¸é›† $X$, x = $\Pi_X(y), \forall z \in X, \|y - x\|_2 \leq \|y - z\|_2$
- **Def 6: Convex Function**
- - $f : X \to \mathcal{R}$ ä¸ºå‡¸: 
- - å¯¹å‡¸é›† $X$, $\forall x, y \in X, \forall \alpha \in [0, 1],f((1-\alpha)x + \alpha y) \leq (1-\alpha)f(x) + \alpha f(y)$
- **Def 7: Concave Function**
- - å¯¹å‡¹é›† $X$, $\forall x, y \in X, \forall \alpha \in [0, 1],f((1-\;alpha)x + \alpha y) \geq (1-\alpha)f(x) + \alpha f(y)$
- **Theorem 2:**
- - å‡½æ•°fä¸ºå‡¸å‡½æ•°å½“ä¸”ä»…å½“dom f ä¸ºå‡¸ä¸”ä»¥ä¸‹ä»»ä¸€æ¡ä»¶æˆç«‹: 
- - $\forall x, y$ in dom f, and $\alpha \in [0, 1]$:
- - é›¶é˜¶æ¡ä»¶:$f((1-\alpha)x + \alpha y) \leq (1-\alpha)f(x) + \alpha f(y)$
- - ä¸€é˜¶æ¡ä»¶:$f(x) + <\nabla f(x),(y-x)> \leq f(y)$
- - äºŒé˜¶æ¡ä»¶:$\nabla^2 f(x) \succeq 0$
- **Theorem 3: Jensen's Inequality**
- - å¯¹å‡¸å‡½æ•°f, *$f(E[X]) \leq E[f(X)]$*
- - Prove:$f(\theta_1x_1 +â€¦â€¦+ \theta_kx_k) \leq \theta_1f(x_1) +â€¦â€¦+ \theta_kf(x_k)$
# Part 2. Convex Optimization Problem
## Setup
- We adopt a minimization language

$$\begin{aligned}\min\quad &f(\mathbf{x})\\\mathbf{s.t.} \quad &g_i(\mathbf{x})\leq0,&i=1,\cdots,m\\
&\mathbf{a}_i^\top\mathbf{x}=b_i,&i=1,\cdots,n\end{aligned}$$

- optimization variable $\mathbf{x}\in\mathbb{R}^d$
- convex objective function: $f:\mathbb{R}^d\mapsto\mathbb{R}$ 
- convex inequality constraints: $g_1,\ldots,g_m$
## Subgradients
**Definition 8 (Subgradient).** 
- Let $f : \chi \rightarrow \mathbb{R}$ be a proper function and let $x \in \chi \subseteq \mathbb{R}^d$. A vector $g \in \mathbb{R}^d$ is called a subgradient of $f$ at $x$ if
$$
f(y) \geq f(x) + \langle g, y - x \rangle, \text{ for all } y \in \mathbb{R}^d.
$$


**Definition 9 (Subdifferential).** 
- The set of all subgradients of $f$ at $x$ is called the subdifferential of $f$ at $x$ and is denoted by $\partial f(x)$,
$$
\partial f(x) = \{ g \in \mathbb{R}^d \mid f(y) \geq f(x) + \langle g, y - x \rangle, \text{ for all } y \in \mathbb{R}^d \}.
$$
- $g \in \partial f(x)$è¡¨ç¤ºç›´çº¿*$f(x)+<g,y-x>$*åœ¨$f$ä¸‹æ–¹
  
**Theorem 4:Relationship between Lipschitznessand bounded subgradient**
- å‡¸å‡½æ•°$f:\mathcal{X}\to\mathbb{R}$. 
- - $(i)$ Lipschitzness: $| f( \mathbf{x} ) - f( \mathbf{y} ) | \leq G\| \mathbf{x} - \mathbf{y} \| \textit{for any x}, \mathbf{y} \in \mathcal{X} .$ 
- - $(ii)$ Bounded subgradient: $\|\mathbf{g}\|_*\leq G$ for any g$\in\partial f(\mathbf{x}),\mathbf{x}\in\mathcal{X}.$ 
- - å¯¹äºä»¥ä¸Šä¸¤æ¡,æœ‰:
- - $(a)(ii)\Rightarrow(i).$
- - $( b) \textit{ if X is open,  then }( i) \Leftrightarrow ( ii) .$

**Theorem 5: Existence of Subgradient**
- $Let\,f:\mathcal{X}\mapsto\mathbb{R}$ be a proper function and assume $\mathcal{X}$ is convex.
If for any $\mathbf{x}\in\mathcal{X},its$ subgradients exist, then f is convex.
- - å‡¸å‡½æ•°çš„å……åˆ†æ¡ä»¶
- - åä¹‹ä¸ç„¶, å¦‚$f(x)=\begin{cases}-\sqrt{x},&x\geq0\\\infty,&\text{else}\end{cases},$
- - åªè€ƒè™‘å¯è¡ŒåŸŸå†…éƒ¨çš„ç‚¹æ—¶,åä¹‹åŒæ ·æˆç«‹

**Theorem 6** å¯¹å‡¸å‡½æ•° $\,f:\mathcal{X}\mapsto\mathbb{R}$ å’Œå‡¸é›† $\mathcal{X}$ is convex.
åˆ™ä»»æ„ interior point $\mathbf{x}\in int(\mathcal{X}), \partial f(x)$ éç©º.

**Theorem 7: Subgradientçš„è®¡ç®—**
- Let $f: \mathcal{X} \mapsto \mathbb{R}$ be a proper and convex function and assume $\mathcal{X}$ is convex.
1. If $f$ is differentiable $at$ $\mathbf{x} , then$ $\partial f( \mathbf{x} ) = \{ \nabla f( \mathbf{x} ) \} .$
2. Conversely, if f has a unique subgradient, then it is differentiable at x and
$\partial f(\mathbf{x})=\{\nabla f(\mathbf{x})\}.$

## Why Convexity?
- Local to Global Phenomenon
- æ¢¯åº¦:æä¾›å‡½æ•°åœ¨æŸç‚¹çš„å±€éƒ¨ä¿¡æ¯
- Subgradient:æä¾›å‡½æ•°åœ¨æŸç‚¹çš„å…¨å±€ä¿¡æ¯


**Theorem 8**
- è®¾ ğ‘“ æ˜¯å‡¸å‡½æ•°ã€‚å¦‚æœğ‘¥æ˜¯ ğ‘“ çš„å±€éƒ¨æœ€å°ç‚¹ï¼Œåˆ™ ğ‘¥ æ˜¯ ğ‘“ çš„å…¨å±€æœ€å°ç‚¹ã€‚
- Proof:
$$f(\mathbf{x})\leq f((1-\gamma)\mathbf{x}+\gamma\mathbf{y})\leq(1-\gamma)f(\mathbf{x})+\gamma f(\mathbf{y}),$$
- - Where $\gamma$ is small enough

# Part 3. Optimality Condition
## Fermatâ€™s Optimality Condition
**Theorem 9 (Fermat's Optimality Condition)**
- $Let f: \mathbb{R} ^d\to ( - \infty , \infty ] \textit{be a proper convex function. Then}$

$$\mathbf{x}^\star\in\operatorname{argmin}\{f(\mathbf{x})\mid\mathbf{x}\in\mathbb{R}^d\}\quad\textit{if and only if} \quad \mathbf{0}\in\partial f(\mathbf{x}^{\star})$$
- - Proof:
$$\begin{aligned}&f(\mathbf{x})\geq f(\mathbf{x}^\star)\\&f(\mathbf{x})\geq f\left(\mathbf{x}^\star\right)+\langle\mathbf{g},\mathbf{x}-\mathbf{x}^\star\rangle,\mathbf{g}\in\partial f(\mathbf{x}^\star)\end{aligned}$$
## First-order Optimality Condition[Constrained Case]
**Theorem 10 (First-order Optimality Condition)**

- Let $\mathcal{X}$ a closed convex set ,å‡¸å‡½æ•°åœ¨$\mathcal{X}$ä¸Šå¯å¯¼. Then $\mathbf{x}^{\star}\in\arg\min_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x})$ if and only if there exists g$\in \partial f( \mathbf{x} ^{\star }) \textit{such that}$

$$\langle\mathbf{g},\mathbf{x}-\mathbf{x}^\star\rangle\geq0,\forall\mathbf{x}\in\mathcal{X}.$$
- Proof: [ç”±Fermat'sç›´æ¥æ¨å¯¼å¾—åˆ°,åŠ å…¥æŒ‡ç¤ºå‡½æ•°ä½¿å…¶å˜ä¸ºæ— çº¦æŸçš„ç›®æ ‡å‡½æ•°h(x)]
- $h(\mathbf{x})\triangleq f(\mathbf{x})+\delta_{\mathcal{X}}(\mathbf{x})$
- - $\partial \delta_{\mathcal{X}}(\mathbf{x}) = N_{\mathcal{X}}(\mathbf{x})=\partial f(\mathbf{x})=\{\mathbf{g}\mid\langle\mathbf{g},\mathbf{y}-\mathbf{x}\rangle\leq0,\forall\mathbf{y}\in\mathcal{X}\}.$
- - $\partial h(\mathbf{x})=\partial f(\mathbf{x})+N_{\mathcal{X}}(\mathbf{x})$

## Some Corollaries
**KKT Conditions**

è€ƒè™‘æœ€å°åŒ–é—®é¢˜
$$\begin{array}{rl}\min&f(\mathbf{x})\\s.t.&g_i(\mathbf{x})\leq0,&i\in[m],\end{array}(1)$$
å…¶ä¸­ $f,g_{1},g_{2},\ldots,g_{m}$ æ˜¯å®å€¼å‡¸å‡½æ•°ã€‚

1. è®° $\textit{x}^{\star}$ ä¸º(1)çš„æœ€ä¼˜è§£ï¼Œå‡è®¾Slateræ¡ä»¶æ»¡è¶³ã€‚åˆ™å­˜åœ¨ $\lambda_{1},\lambda_{2},\ldots,\lambda_{m}\geq0$ ä½¿å¾—

$$\mathbf{0}\in\partial f\left(\mathbf{x}^\star\right)+\sum_{i=1}^m\lambda_i\partial g_i\left(\mathbf{x}^\star\right)(2)$$
$$\lambda_ig_i\left(\mathbf{x}^\star\right)=0,\quad i\in[m].(3)$$

2. è‹¥å¯¹äºæŸä¸ª $\mathbf{x}^{\star}$ å­˜åœ¨ $\lambda_{1},\lambda_{2},\ldots,\lambda_{m}\geq0$ æ»¡è¶³æ¡ä»¶(2)å’Œ(3)ï¼Œåˆ™$\mathbf{x}^{\star}$ä¹Ÿæ˜¯é—®é¢˜(1)çš„æœ€ä¼˜è§£ã€‚

# Part 4. Function Properties
## Smoothness
**Def 1: Continuity**
-  å‡½æ•° f: $\mathbb{R} ^n\to \mathbb{R} ^m$ is continuous at $\mathbf{x}\in$dom
$f$ if for all $\epsilon>0$ there exists a $\delta>0$ with $\mathbf{y}\in$dom $f$,such that
$$\|\mathbf{y}-\mathbf{x}\|_2\leq\delta\Rightarrow\|f(\mathbf{y})-f(\mathbf{x})\|_2\leq\epsilon.$$

**Def 2: Lipschitz Continuity**
- A function $f:\mathbb{R}^n\to\mathbb{R}^m$ is $G$-Lipschitz-continuous if for all x, y$\in\operatorname{dom}f$,
$$\left\|f(\mathbf{x})-f(\mathbf{y})\right\|\leq G\left\|\mathbf{x}-\mathbf{y}\right\|.$$

**Def 3: Smoothness**
- A function $f\mathrm{~is~}L\text{-smooth with respect to the }\|\cdot\|$
norm if, for any $\mathbf{x},\mathbf{y}\in\operatorname{dom}f$,
$$\|\nabla f(\mathbf{x})-\nabla f(\mathbf{y})\|_*\leq L\|\mathbf{x}-\mathbf{y}\|.$$
$||\cdot||_*$è¡¨ç¤ºå¯¹å¶èŒƒæ•°.åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸‹,ä¸º$l_2$èŒƒæ•°,ä¸Šå¼æˆä¸º:
$$\|\nabla f(\mathbf{x})-\nabla f(\mathbf{y})\|_2\leq L\|\mathbf{x}-\mathbf{y}\|_2.$$
- domain $\mathcal{X}$ ä¸Šçš„$\mathcal{L}-$smoothå‡½æ•°é›†åˆè®°ä¸º$C_{\mathcal{L}}^{1,1}(\mathcal{X})$.

**Def 4:**
- Let $\mathcal{X}\subseteq\mathbb{R}^d.$ $C_L^{a,b}(\mathcal{X})$å®šä¹‰å¦‚ä¸‹:

$(i)$ any $f\in C_L^{a,b}(\mathcal{X})$ åœ¨$\mathcal{X}.$aæ¬¡è¿ç»­å¯å¾®

$(ii)$ fçš„bé˜¶å¯¼æ•°åœ¨$\mathcal{X}$ä¸ŠLipschitzè¿ç»­,Lipschitzå¸¸æ•°ä¸ºğ¿
$$\left\|\nabla^bf(\mathbf{x})-\nabla^bf(\mathbf{y})\right\|_*\leq L\|\mathbf{x}-\mathbf{y}\|,\:\forall\mathbf{x},\mathbf{y}\in\mathcal{X}.$$


**Lemma 1 (Descent Lemma)**
- fæ˜¯å‡¸é›† $\mathcal{X}$ä¸Šçš„L-smoothå‡½æ•°.åˆ™å¯¹ä»»æ„ $\mathbf{x},\mathbf{y}\in\mathcal{X}$,
$$f(\mathbf{y}) \leq f(\mathbf{x})+\nabla f(\mathbf{x})^{\top}(\mathbf{y}-\mathbf{x})+\frac{L}{2}\|\mathbf{y}-\mathbf{x}\|^{2}.$$
- $\textbf{Proof:}$
$$ f(y) - f(x) = \int_0^1 \langle \nabla f(x + t(y - x)), y - x \rangle dt \quad \text{(calculus)} \\[10pt]
\Rightarrow \quad f(y) - f(x) - \langle \nabla f(x), y - x \rangle = \int_0^1 \langle \nabla f(x + t(y - x)) - \nabla f(x), y - x \rangle dt \\[10pt]
\text{(Cauchy-Schwarz)} \quad \leq \int_0^1 \| \nabla f(x + t(y - x)) - \nabla f(x) \| \, \| y - x \| \, dt \\[10pt]
\text{(smoothness)} \quad \leq L \| y - x \|^2 \int_0^1 t \, dt \leq \frac{L}{2} \| y - x \|^2 \quad \square $$

**Theorem 2 (First-order Characterizations of  L -smoothness)**
- Let  $f : \mathcal{X} \rightarrow \mathbb{R}$  be a convex function, differentiable over  $\mathcal{X}$ . Then the following claims are equivalent:
- $(i)  f \, is \, L -smooth.$
- $(ii)  f(y) \leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \| x - y \|^2  \, for \, all  x, y \in \mathcal{X} .$
- $(iii)  f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} \| \nabla f(x) - \nabla f(y) \|_*^2  for \, all \, x, y \in \mathcal{X} .$
- $(iv)  \langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \frac{1}{L} \| \nabla f(x) - \nabla f(y) \|_*^2 \, for \, all \, x, y \in \mathcal{X} .$
- $(v)  f(\lambda x + (1 - \lambda) y) \geq \lambda f(x) + (1 - \lambda) f(y) - \frac{L}{2} \lambda (1 - \lambda) \| x - y \|^2  for \, any \, x, y \in \mathcal{X} \, and \, \lambda \in [0, 1] .$

**Theorem 3 (Second-order Characterization of $L$-smoothness)**
- Let f be a twice continuously differentiable function over $\mathbb{R}^d.$ Then for a given $L\geq0,L$-smoothness $w. r. t.$ the $l_{p}$-norm $(p\in[1,\infty])$ is equivalent to

$$\left\|\nabla^2f(\mathbf{x})\right\|_{op,p}\leq L,$$
  for any x$\in\mathbb{R}^d.$


## Strong Convexity
**Definition 5 (Strong Convexity).**
- A function f is Ïƒ-strongly convex if, for any $\mathbf{x},\mathbf{y}\in\operatorname{dom}f\mathrm{~and~}\lambda\in[0,1],\\f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})\leq\lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y})-\frac\sigma2\lambda(1-\lambda)\|\mathbf{x}-\mathbf{y}\|^2.$

**Theorem 3** (*First-order Characterizations of Strong Convexity*)
- Let $f$ be a proper closed and convex function. Then for a given $\sigma > 0$, the followings are equivalent:

1. $f$ is $\sigma$-strongly convex.

2. For any $\mathbf{x} \in \operatorname{dom}(\partial f)$, $\mathbf{y} \in \operatorname{dom}(f)$ and $g \in \partial f(\mathbf{x})$,
   
   <span style="background-color: #D0E7FF; color: black; padding: 10px;">$
   f(\mathbf{y}) \geq f(\mathbf{x}) + \langle g, \mathbf{y} - \mathbf{x} \rangle + \frac{\sigma}{2} \| \mathbf{y} - \mathbf{x} \|^2.$
   *(commonly used)*

1. For any $\mathbf{x}, \mathbf{y} \in \operatorname{dom}(\partial f)$, and $g_{\mathbf{x}} \in \partial f(\mathbf{x})$, $g_{\mathbf{y}} \in \partial f(\mathbf{y})$,
   $$
   \langle g_{\mathbf{x}} - g_{\mathbf{y}}, \mathbf{x} - \mathbf{y} \rangle \geq \sigma \| \mathbf{x} - \mathbf{y} \|^2.
   $$

2. Function $f(\cdot) - \frac{\sigma}{2} \| \cdot \|^2$ is convex.

**Theorem 4.** 
- $\mathcal{X}$ ä¸ºæ¬§æ°ç©ºé—´. åˆ™å‡½æ•° $f$ ä¸º $\sigma$-strongly convex å½“ä¸”ä»…å½“ $f(\cdot) - \frac{\sigma}{2} \| \cdot \|^2$ is convex.

*f is "as least as convex" as a quadratic function.*

**Theorem 5** (*Second-order Characterization of Strong Convexity*). 
- Let $\mathcal{X}$ be a Euclidean space. Then $f$ is $\sigma$-strongly convex if and only if for any $\mathbf{x}, \mathbf{w} \in \mathcal{X}$,

$$\mathbf{w}^\top \nabla^2 f(\mathbf{x}) \mathbf{w} \geq \sigma \|\mathbf{w}\|^2.$$

*a more familiar form:* $\|\mathbf{w}\|^2_{\nabla^2 f(\mathbf{x})}$.

Furthermore, when using $\ell_2$-norm, it is equivalent to $\nabla^2 f(\mathbf{x}) \succeq \sigma I$.

**Theorem 6.** 
- Let $f$ be a proper closed and $\sigma$-strongly convex function. Then

- - $f$ æœ‰å”¯ä¸€æå°å€¼ $\mathbf{x}^\star$.
- - $f(\mathbf{x}) - f(\mathbf{x}^\star) \geq \frac{\sigma}{2} \| \mathbf{x} - \mathbf{x}^\star \|^2,\forall \mathbf{x} \in \operatorname{dom}(f)$.

## Strongly Convex and Smooth
- If function $f$ is both $\sigma$-strongly convex and $L$-smooth with respect to $\ell_2$-norm, then

- - $\sigma I \preceq \nabla^2 f(\mathbf{x}) \preceq L I$
- - $f$ is $\gamma$-*well-conditioned* where $\gamma \triangleq \frac{\sigma}{L} \leq 1$ is called the condition number.

**Theorem 7** (*Conjugate Correspondence*). 
- è€ƒè™‘å…±è½­å‡½æ•°:
$$
f^*(\mathbf{y}) \triangleq \max_{\mathbf{x} \in \mathcal{X}} \left\{ \langle \mathbf{y}, \mathbf{x} \rangle - f(\mathbf{x}) \right\}.
$$

(a) If the function $f$ is convex and $\frac{1}{\sigma}$-smooth with respect to the norm $\| \cdot \|$, then its conjugate $f^*$ is $\sigma$-strongly convex with respect to the dual norm $\| \cdot \|_*$.

(b) If $f$ is proper closed and $\sigma$-strongly convex with respect to the norm $\| \cdot \|$, then $f^*$ is $\frac{1}{\sigma}$-smooth with respect to the dual norm $\| \cdot \|_*$.



